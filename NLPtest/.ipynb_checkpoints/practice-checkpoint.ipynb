{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/au/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.873 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title2_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>警方 辟谣 鸟巢 大会 每人 领 5 万 仍 有 老人 坚持 进京</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>深圳 GDP 首 超 香港 深圳 统计局 辟谣 只是 差距 在 缩小</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>GDP 首 超 香港 深圳 澄清 还 差 一点点</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>去年深圳GDP首超香港？深圳统计局辟谣：还差611亿</td>\n",
       "      <td>去年 深圳 GDP 首 超 香港 深圳 统计局 辟谣 还 差 611 亿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>吃了30年食用油才知道，一片大蒜轻松鉴别地沟油</td>\n",
       "      <td>吃 了 30 年 食用油 才 知道 一片 大蒜 轻松 鉴别 地沟油</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    title2_zh                      title2_tokenized\n",
       "0    警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京     警方 辟谣 鸟巢 大会 每人 领 5 万 仍 有 老人 坚持 进京\n",
       "1   深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小    深圳 GDP 首 超 香港 深圳 统计局 辟谣 只是 差距 在 缩小\n",
       "2        GDP首超香港？深圳澄清：还差一点点……              GDP 首 超 香港 深圳 澄清 还 差 一点点\n",
       "3  去年深圳GDP首超香港？深圳统计局辟谣：还差611亿  去年 深圳 GDP 首 超 香港 深圳 统计局 辟谣 还 差 611 亿\n",
       "4     吃了30年食用油才知道，一片大蒜轻松鉴别地沟油     吃 了 30 年 食用油 才 知道 一片 大蒜 轻松 鉴别 地沟油"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba.posseg as pseg\n",
    "import keras\n",
    "\n",
    "train = pd.read_csv('/home/au/文件/Data/NLP Data/train.csv', encoding = 'utf-8')\n",
    "train = train.astype(str)\n",
    "cols = ['title1_zh', 'title2_zh', 'label']\n",
    "train = train.loc[0:100, cols]\n",
    "# train.head(3)\n",
    "\n",
    "def jieba_tokenizer(text):\n",
    "    words = pseg.cut(text)\n",
    "    return ' '.join([\n",
    "        word for word, flag in words if flag != 'x'])\n",
    "\n",
    "train['title1_tokenized'] = \\\n",
    "    train.loc[0:, 'title1_zh'] \\\n",
    "         .apply(jieba_tokenizer)\n",
    "train['title2_tokenized'] = \\\n",
    "    train.loc[0:, 'title2_zh'] \\\n",
    "         .apply(jieba_tokenizer)\n",
    "\n",
    "train.iloc[0:, [0, 3]].head()\n",
    "train.iloc[0:, [1, 4]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maxword = 10000\n",
    "tokenizer = keras \\\n",
    "    .preprocessing \\\n",
    "    .text \\\n",
    "    .Tokenizer(num_words=maxword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0               2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗\n",
      "1        你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
      "2        你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
      "3        你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
      "4                             用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油\n",
      "5        你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
      "6                                      吃 榴莲 的 禁忌 吃 错会 致命\n",
      "7        你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
      "8              旅行 青蛙 居然 是 一款 生育 意愿 测试 器 大家 还是 玩 珠宝 V 课 吧\n",
      "9                             用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油\n",
      "10                            用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油\n",
      "11                            用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油\n",
      "12                            用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油\n",
      "13                            用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油\n",
      "14             飞机 就要 起飞 一个 男人 在 机舱 口 跪下 这 是 今天 最 催泪 的 一幕\n",
      "15                飞机 就要 起飞 一个 男人 在 机舱 口 跪下 这 是 最 催泪 的 一幕\n",
      "16                           男人 在 机舱 口 跪下 原来 一切 都 只 因为 爱\n",
      "17             飞机 就要 起飞 一个 男人 在 机舱 口 跪下 这 是 今天 最 催泪 的 一幕\n",
      "18            飞机 就要 起飞 一个 男人 在 机舱 口 跪下 这 是 见 过 最 催泪 的 一幕\n",
      "19              健康 过大年 还 在 逗 孩子 喝酒 儿童 喝酒 的 危害 多 大 你 知道 吗\n",
      "20              健康 过大年 还 在 逗 孩子 喝酒 儿童 喝酒 的 危害 多 大 你 知道 吗\n",
      "21              健康 过大年 还 在 逗 孩子 喝酒 儿童 喝酒 的 危害 多 大 你 知道 吗\n",
      "22                            农民 铜川市 耀州区 农民 每亩 地 补助 多少 钱\n",
      "23                            农民 铜川市 耀州区 农民 每亩 地 补助 多少 钱\n",
      "24     吃秀 美好 的 一天 从 早餐 开始 续集 这 会 灯 好多 上 个 一会 黑 一会 白 本...\n",
      "25     吃秀 美好 的 一天 从 早餐 开始 续集 这 会 灯 好多 上 个 一会 黑 一会 白 本...\n",
      "26     吃秀 美好 的 一天 从 早餐 开始 续集 这 会 灯 好多 上 个 一会 黑 一会 白 本...\n",
      "27     吃秀 美好 的 一天 从 早餐 开始 续集 这 会 灯 好多 上 个 一会 黑 一会 白 本...\n",
      "28     吃秀 美好 的 一天 从 早餐 开始 续集 这 会 灯 好多 上 个 一会 黑 一会 白 本...\n",
      "29     吃秀 美好 的 一天 从 早餐 开始 续集 这 会 灯 好多 上 个 一会 黑 一会 白 本...\n",
      "                             ...                        \n",
      "71                            芯片 公司 2000 万 净利润 就 上市 是 谣言\n",
      "72                        艺根 新材 对 公司 拟转 板 上市 虚假 报道 进行 澄清\n",
      "73                            芯片 公司 2000 万 净利润 可 上市 系 谣言\n",
      "74                鹿晗 和 迪丽 热巴 将要 参加 我们 相爱 吧 双方 粉丝 表示 好 期待\n",
      "75                鹿晗 和 迪丽 热巴 将要 参加 我们 相爱 吧 双方 粉丝 表示 好 期待\n",
      "76                       可口可乐 承认 旗下 果粒 橙 含有 美国 禁用 农药 多菌灵\n",
      "77                鹿晗 迪丽 热巴 携手 参加 我们 相爱 吧 网友 直呼 真的 吗 好 期待\n",
      "78                      陆地 夫妇 鹿晗 和 迪丽 热巴 将 携手 参加 我们 相爱 吧\n",
      "79             十一 鹿晗 迪丽 热巴 的 点滴 小 趣事 热 吧 你 男友 邀 你 参加 约 吧\n",
      "80                            鹿晗 迪丽 热巴 要 在 我们 相爱 吧 再续 情缘\n",
      "81                                     俄罗斯 军方 捕获 到 的 外星人\n",
      "82                                    七种 食物 不能 和 鸡蛋 一起 吃\n",
      "83                                   柿子 不能 和 什么 一起 吃 同 吃\n",
      "84             吃 完 鸡蛋 后 不要 立即 吃 柿子 鸡蛋 千万 不能 跟 这些 东西 一起 吃\n",
      "85                           鸡蛋 柿子 不能 同 吃 5 种 食物 是 鸡蛋 克星\n",
      "86                         男人 每 7 秒 就 想 一次性 每 小时 超 500 次\n",
      "87                        男性 平均 每 7 秒 就 会 想到 和 性 有关 的 东西\n",
      "88                                     男性 每 7 秒 就 会 想到 性\n",
      "89                               深夜 短剧 男人 每 7 秒 就 想到 一次性\n",
      "90                                   男 性 趣谈 每 7 秒钟 想 一次性\n",
      "91             6 月 蚊子 无法无天 蚊香 无用 不 环保 教 你 一招 妙计 蠢 蚊子 哭死了\n",
      "92                           研究 表明 男人 平均 每 七秒钟 就 会 想到 性爱\n",
      "93             六月 蚊子 多多 千万别 用 蚊香 教 你 一招 家里 屋子 的 蚊子 全都 死光\n",
      "94           蚊子 太 多 花露水 蚊香 不管 用 老婆 教 我 一招 3 秒 千米 蚊虫 全 死光\n",
      "95                           精虫 上脑 男人 每 7 秒 想 做爱 女人 更 夸张\n",
      "96                            媒体 曝 英国 多个 实验室 秘密 实验 人兽 杂交\n",
      "97               千万别 点 蚊香 了 对 孩子 不好 嫂子 教 我 一招 秒钟 蚊子 全 死光\n",
      "98                              英国 多个 实验室 秘密 进行 人兽 杂交 实验\n",
      "99                      曝 英国 多个 实验室 秘密 进行 人兽 杂交 实验 达 3 年\n",
      "100                                     英国 爆 人兽 杂交 秘密 实验\n",
      "Length: 202, dtype: object\n"
     ]
    }
   ],
   "source": [
    "corpus_x1 = train.title1_tokenized\n",
    "corpus_x2 = train.title2_tokenized\n",
    "corpus = pd.concat([\n",
    "    corpus_x1, corpus_x2])\n",
    "corpus.shape\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[318, 319, 234, 320, 321, 322, 235, 323, 184, 324, 9, 325, 12, 27]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_NUM_WORDS = 10000\n",
    "tokenizer = keras \\\n",
    "    .preprocessing \\\n",
    "    .text \\\n",
    "    .Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "x1_train = tokenizer \\\n",
    "    .texts_to_sequences(corpus_x1)\n",
    "x2_train = tokenizer \\\n",
    "    .texts_to_sequences(corpus_x2)\n",
    "len(x1_train)\n",
    "x1_train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2017', '养老保险', '又', '新增', '两项', '农村', '老人', '人人', '可', '申领', '你', '领到', '了', '吗']\n"
     ]
    }
   ],
   "source": [
    "for seq in x1_train[:1]:\n",
    "    print([tokenizer.index_word[idx] for idx in seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 [318, 319, 234, 320, 321]  ...\n",
      "19 [9, 78, 59, 29, 120]  ...\n",
      "19 [9, 78, 59, 29, 120]  ...\n",
      "19 [9, 78, 59, 29, 120]  ...\n",
      "9 [55, 52, 28, 25, 1]  ...\n",
      "19 [9, 78, 59, 29, 120]  ...\n",
      "7 [2, 236, 1, 124, 2]  ...\n",
      "19 [9, 78, 59, 29, 120]  ...\n",
      "16 [97, 328, 237, 64, 329]  ...\n",
      "9 [55, 52, 28, 25, 1]  ...\n"
     ]
    }
   ],
   "source": [
    "for seq in x1_train[:10]:\n",
    "    print(len(seq), seq[:5], ' ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = max([\n",
    "    len(seq) for seq in x1_train])\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 20\n",
    "x1_train = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x1_train, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "x2_train = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x2_train, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 53, 47, 39, 56, 62, 79, 46,\n",
       "       80, 81, 12], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有新聞標題的序列長度皆為 20 !\n"
     ]
    }
   ],
   "source": [
    "for seq in x1_train + x2_train:\n",
    "    assert len(seq) == 20\n",
    "    \n",
    "print(\"所有新聞標題的序列長度皆為 20 !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  53,  47,  39,\n",
       "         56,  62,  79,  46,  80,  81,  12],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  87,  66,  88,  68,  30,  41,\n",
       "         48,  49,   5,   7,  40,  63,  89],\n",
       "       [  0,   0,   0,   0,   0,   0, 202, 203, 204, 205, 206,  51,  10,\n",
       "        207, 208, 108, 130, 209, 210, 108],\n",
       "       [  0,   0,   0,   0, 102, 186,  70,  26, 187,  71, 103, 188, 103,\n",
       "          1, 104,  65, 143,   9, 144,  27],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 154, 111,  85,\n",
       "         86, 155,   2,  69,  40, 156, 124]], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# 定義每一個分類對應到的索引數字\n",
    "label_to_index = {\n",
    "    'unrelated': 0, \n",
    "    'agreed': 1, \n",
    "    'disagreed': 2\n",
    "}\n",
    "\n",
    "# 將分類標籤對應到剛定義的數字\n",
    "y_train = train.label.apply(\n",
    "    lambda x: label_to_index[x])\n",
    "\n",
    "y_train = np.asarray(y_train) \\\n",
    "            .astype('float32')\n",
    "\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = keras \\\n",
    "    .utils \\\n",
    "    .to_categorical(y_train)\n",
    "\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection \\\n",
    "    import train_test_split\n",
    "\n",
    "VALIDATION_RATIO = 0.1\n",
    "# 小彩蛋\n",
    "RANDOM_STATE = 9527\n",
    "\n",
    "x1_train, x1_val, \\\n",
    "x2_train, x2_val, \\\n",
    "y_train, y_val = \\\n",
    "    train_test_split(\n",
    "        x1_train, x2_train, y_train, \n",
    "        test_size=VALIDATION_RATIO, \n",
    "        random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "----------\n",
      "x1_train: (90, 20)\n",
      "x2_train: (90, 20)\n",
      "y_train : (90, 3)\n",
      "----------\n",
      "x1_val:   (11, 20)\n",
      "x2_val:   (11, 20)\n",
      "y_val :   (11, 3)\n",
      "----------\n",
      "Test Set\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set\")\n",
    "print(\"-\" * 10)\n",
    "print(f\"x1_train: {x1_train.shape}\")\n",
    "print(f\"x2_train: {x2_train.shape}\")\n",
    "print(f\"y_train : {y_train.shape}\")\n",
    "\n",
    "print(\"-\" * 10)\n",
    "print(f\"x1_val:   {x1_val.shape}\")\n",
    "print(f\"x2_val:   {x2_val.shape}\")\n",
    "print(f\"y_val :   {y_val.shape}\")\n",
    "print(\"-\" * 10)\n",
    "print(\"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NUM_EMBEDDING_DIM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-70cf388f89ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m embedding_layer = layers.Embedding(\n\u001b[0;32m----> 3\u001b[0;31m     MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'NUM_EMBEDDING_DIM' is not defined"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "embedding_layer = layers.Embedding(\n",
    "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
