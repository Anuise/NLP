{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title2_zh</th>\n",
       "      <th>title2_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京</td>\n",
       "      <td>警方 辟谣 鸟巢 大会 每人 领 5 万 仍 有 老人 坚持 进京</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小</td>\n",
       "      <td>深圳 GDP 首 超 香港 深圳 统计局 辟谣 只是 差距 在 缩小</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GDP首超香港？深圳澄清：还差一点点……</td>\n",
       "      <td>GDP 首 超 香港 深圳 澄清 还 差 一点点</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>去年深圳GDP首超香港？深圳统计局辟谣：还差611亿</td>\n",
       "      <td>去年 深圳 GDP 首 超 香港 深圳 统计局 辟谣 还 差 611 亿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>吃了30年食用油才知道，一片大蒜轻松鉴别地沟油</td>\n",
       "      <td>吃 了 30 年 食用油 才 知道 一片 大蒜 轻松 鉴别 地沟油</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    title2_zh                      title2_tokenized\n",
       "0    警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京     警方 辟谣 鸟巢 大会 每人 领 5 万 仍 有 老人 坚持 进京\n",
       "1   深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小    深圳 GDP 首 超 香港 深圳 统计局 辟谣 只是 差距 在 缩小\n",
       "2        GDP首超香港？深圳澄清：还差一点点……              GDP 首 超 香港 深圳 澄清 还 差 一点点\n",
       "3  去年深圳GDP首超香港？深圳统计局辟谣：还差611亿  去年 深圳 GDP 首 超 香港 深圳 统计局 辟谣 还 差 611 亿\n",
       "4     吃了30年食用油才知道，一片大蒜轻松鉴别地沟油     吃 了 30 年 食用油 才 知道 一片 大蒜 轻松 鉴别 地沟油"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import jieba.posseg as pseg\n",
    "import keras\n",
    "\n",
    "train = pd.read_csv('/home/au/文件/Data/NLP/train.csv', encoding = 'utf-8')\n",
    "train = train.astype(str)\n",
    "cols = ['title1_zh', 'title2_zh', 'label']\n",
    "train = train.loc[0:100, cols]\n",
    "# train.head(3)\n",
    "\n",
    "def jieba_tokenizer(text):\n",
    "    words = pseg.cut(text)\n",
    "    return ' '.join([\n",
    "        word for word, flag in words if flag != 'x'])\n",
    "\n",
    "train['title1_tokenized'] = \\\n",
    "    train.loc[0:, 'title1_zh'] \\\n",
    "         .apply(jieba_tokenizer)\n",
    "train['title2_tokenized'] = \\\n",
    "    train.loc[0:, 'title2_zh'] \\\n",
    "         .apply(jieba_tokenizer)\n",
    "\n",
    "train.iloc[0:, [0, 3]].head()\n",
    "train.iloc[0:, [1, 4]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maxword = 10000\n",
    "tokenizer = keras \\\n",
    "    .preprocessing \\\n",
    "    .text \\\n",
    "    .Tokenizer(num_words=maxword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0               2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗\n",
      "1        你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
      "2        你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
      "3        你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
      "4                             用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油\n",
      "5        你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
      "6                                      吃 榴莲 的 禁忌 吃 错会 致命\n",
      "7        你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港\n",
      "8              旅行 青蛙 居然 是 一款 生育 意愿 测试 器 大家 还是 玩 珠宝 V 课 吧\n",
      "9                             用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油\n",
      "10                            用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油\n",
      "11                            用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油\n",
      "12                            用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油\n",
      "13                            用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油\n",
      "14             飞机 就要 起飞 一个 男人 在 机舱 口 跪下 这 是 今天 最 催泪 的 一幕\n",
      "15                飞机 就要 起飞 一个 男人 在 机舱 口 跪下 这 是 最 催泪 的 一幕\n",
      "16                           男人 在 机舱 口 跪下 原来 一切 都 只 因为 爱\n",
      "17             飞机 就要 起飞 一个 男人 在 机舱 口 跪下 这 是 今天 最 催泪 的 一幕\n",
      "18            飞机 就要 起飞 一个 男人 在 机舱 口 跪下 这 是 见 过 最 催泪 的 一幕\n",
      "19              健康 过大年 还 在 逗 孩子 喝酒 儿童 喝酒 的 危害 多 大 你 知道 吗\n",
      "20              健康 过大年 还 在 逗 孩子 喝酒 儿童 喝酒 的 危害 多 大 你 知道 吗\n",
      "21              健康 过大年 还 在 逗 孩子 喝酒 儿童 喝酒 的 危害 多 大 你 知道 吗\n",
      "22                            农民 铜川市 耀州区 农民 每亩 地 补助 多少 钱\n",
      "23                            农民 铜川市 耀州区 农民 每亩 地 补助 多少 钱\n",
      "24     吃秀 美好 的 一天 从 早餐 开始 续集 这 会 灯 好多 上 个 一会 黑 一会 白 本...\n",
      "25     吃秀 美好 的 一天 从 早餐 开始 续集 这 会 灯 好多 上 个 一会 黑 一会 白 本...\n",
      "26     吃秀 美好 的 一天 从 早餐 开始 续集 这 会 灯 好多 上 个 一会 黑 一会 白 本...\n",
      "27     吃秀 美好 的 一天 从 早餐 开始 续集 这 会 灯 好多 上 个 一会 黑 一会 白 本...\n",
      "28     吃秀 美好 的 一天 从 早餐 开始 续集 这 会 灯 好多 上 个 一会 黑 一会 白 本...\n",
      "29     吃秀 美好 的 一天 从 早餐 开始 续集 这 会 灯 好多 上 个 一会 黑 一会 白 本...\n",
      "                             ...                        \n",
      "71                            芯片 公司 2000 万 净利润 就 上市 是 谣言\n",
      "72                        艺根 新材 对 公司 拟转 板 上市 虚假 报道 进行 澄清\n",
      "73                            芯片 公司 2000 万 净利润 可 上市 系 谣言\n",
      "74                鹿晗 和 迪丽 热巴 将要 参加 我们 相爱 吧 双方 粉丝 表示 好 期待\n",
      "75                鹿晗 和 迪丽 热巴 将要 参加 我们 相爱 吧 双方 粉丝 表示 好 期待\n",
      "76                       可口可乐 承认 旗下 果粒 橙 含有 美国 禁用 农药 多菌灵\n",
      "77                鹿晗 迪丽 热巴 携手 参加 我们 相爱 吧 网友 直呼 真的 吗 好 期待\n",
      "78                      陆地 夫妇 鹿晗 和 迪丽 热巴 将 携手 参加 我们 相爱 吧\n",
      "79             十一 鹿晗 迪丽 热巴 的 点滴 小 趣事 热 吧 你 男友 邀 你 参加 约 吧\n",
      "80                            鹿晗 迪丽 热巴 要 在 我们 相爱 吧 再续 情缘\n",
      "81                                     俄罗斯 军方 捕获 到 的 外星人\n",
      "82                                    七种 食物 不能 和 鸡蛋 一起 吃\n",
      "83                                   柿子 不能 和 什么 一起 吃 同 吃\n",
      "84             吃 完 鸡蛋 后 不要 立即 吃 柿子 鸡蛋 千万 不能 跟 这些 东西 一起 吃\n",
      "85                           鸡蛋 柿子 不能 同 吃 5 种 食物 是 鸡蛋 克星\n",
      "86                         男人 每 7 秒 就 想 一次性 每 小时 超 500 次\n",
      "87                        男性 平均 每 7 秒 就 会 想到 和 性 有关 的 东西\n",
      "88                                     男性 每 7 秒 就 会 想到 性\n",
      "89                               深夜 短剧 男人 每 7 秒 就 想到 一次性\n",
      "90                                   男 性 趣谈 每 7 秒钟 想 一次性\n",
      "91             6 月 蚊子 无法无天 蚊香 无用 不 环保 教 你 一招 妙计 蠢 蚊子 哭死了\n",
      "92                           研究 表明 男人 平均 每 七秒钟 就 会 想到 性爱\n",
      "93             六月 蚊子 多多 千万别 用 蚊香 教 你 一招 家里 屋子 的 蚊子 全都 死光\n",
      "94           蚊子 太 多 花露水 蚊香 不管 用 老婆 教 我 一招 3 秒 千米 蚊虫 全 死光\n",
      "95                           精虫 上脑 男人 每 7 秒 想 做爱 女人 更 夸张\n",
      "96                            媒体 曝 英国 多个 实验室 秘密 实验 人兽 杂交\n",
      "97               千万别 点 蚊香 了 对 孩子 不好 嫂子 教 我 一招 秒钟 蚊子 全 死光\n",
      "98                              英国 多个 实验室 秘密 进行 人兽 杂交 实验\n",
      "99                      曝 英国 多个 实验室 秘密 进行 人兽 杂交 实验 达 3 年\n",
      "100                                     英国 爆 人兽 杂交 秘密 实验\n",
      "Length: 202, dtype: object\n"
     ]
    }
   ],
   "source": [
    "corpus_x1 = train.title1_tokenized\n",
    "corpus_x2 = train.title2_tokenized\n",
    "corpus = pd.concat([\n",
    "    corpus_x1, corpus_x2])\n",
    "corpus.shape\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[318, 319, 234, 320, 321, 322, 235, 323, 184, 324, 9, 325, 12, 27]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_NUM_WORDS = 10000\n",
    "tokenizer = keras \\\n",
    "    .preprocessing \\\n",
    "    .text \\\n",
    "    .Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "x1_train = tokenizer \\\n",
    "    .texts_to_sequences(corpus_x1)\n",
    "x2_train = tokenizer \\\n",
    "    .texts_to_sequences(corpus_x2)\n",
    "len(x1_train)\n",
    "x1_train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2017', '养老保险', '又', '新增', '两项', '农村', '老人', '人人', '可', '申领', '你', '领到', '了', '吗']\n"
     ]
    }
   ],
   "source": [
    "for seq in x1_train[:1]:\n",
    "    print([tokenizer.index_word[idx] for idx in seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 [318, 319, 234, 320, 321]  ...\n",
      "19 [9, 78, 59, 29, 120]  ...\n",
      "19 [9, 78, 59, 29, 120]  ...\n",
      "19 [9, 78, 59, 29, 120]  ...\n",
      "9 [55, 52, 28, 25, 1]  ...\n",
      "19 [9, 78, 59, 29, 120]  ...\n",
      "7 [2, 236, 1, 124, 2]  ...\n",
      "19 [9, 78, 59, 29, 120]  ...\n",
      "16 [97, 328, 237, 64, 329]  ...\n",
      "9 [55, 52, 28, 25, 1]  ...\n"
     ]
    }
   ],
   "source": [
    "for seq in x1_train[:10]:\n",
    "    print(len(seq), seq[:5], ' ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = max([\n",
    "    len(seq) for seq in x1_train])\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 20\n",
    "x1_train = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x1_train, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "x2_train = keras \\\n",
    "    .preprocessing \\\n",
    "    .sequence \\\n",
    "    .pad_sequences(x2_train, \n",
    "                   maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有新聞標題的序列長度皆為 20 !\n"
     ]
    }
   ],
   "source": [
    "for seq in x1_train + x2_train:\n",
    "    assert len(seq) == 20\n",
    "    \n",
    "print(\"所有新聞標題的序列長度皆為 20 !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "# 定義每一個分類對應到的索引數字\n",
    "label_to_index = {\n",
    "    'unrelated': 0, \n",
    "    'agreed': 1, \n",
    "    'disagreed': 2\n",
    "}\n",
    "\n",
    "# 將分類標籤對應到剛定義的數字\n",
    "y_train = train.label.apply(\n",
    "    lambda x: label_to_index[x])\n",
    "\n",
    "y_train = np.asarray(y_train) \\\n",
    "            .astype('float32')\n",
    "\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = keras \\\n",
    "    .utils \\\n",
    "    .to_categorical(y_train)\n",
    "\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection \\\n",
    "    import train_test_split\n",
    "\n",
    "VALIDATION_RATIO = 0.1\n",
    "# 小彩蛋\n",
    "RANDOM_STATE = 9527\n",
    "\n",
    "x1_train, x1_val, \\\n",
    "x2_train, x2_val, \\\n",
    "y_train, y_val = \\\n",
    "    train_test_split(\n",
    "        x1_train, x2_train, y_train, \n",
    "        test_size=VALIDATION_RATIO, \n",
    "        random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'units'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-4d7ece37e7e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimpleRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'units'"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "rnn = layers.SimpleRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_t = 0\n",
    "# 細胞 A 會重複執行以下處理\n",
    "for input_t in input_sequence:\n",
    "    output_t = f(input_t, state_t)\n",
    "    state_t = output_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "lstm = layers.LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, seq in enumerate(x1_train[:5]):\n",
    "    print(f\"新聞標題 {i + 1}: \")\n",
    "    print(seq)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "embedding_layer = layers.Embedding(\n",
    "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本參數設置，有幾個分類\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "# 在語料庫裡有多少詞彙\n",
    "MAX_NUM_WORDS = 10000\n",
    "\n",
    "# 一個標題最長有幾個詞彙\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "\n",
    "# 一個詞向量的維度\n",
    "NUM_EMBEDDING_DIM = 256\n",
    "\n",
    "# LSTM 輸出的向量維度\n",
    "NUM_LSTM_UNITS = 128\n",
    "\n",
    "# 建立孿生 LSTM 架構（Siamese LSTM）\n",
    "from keras import Input\n",
    "from keras.layers import Embedding, \\\n",
    "    LSTM, concatenate, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# 分別定義 2 個新聞標題 A & B 為模型輸入\n",
    "# 兩個標題都是一個長度為 20 的數字序列\n",
    "top_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "bm_input = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "\n",
    "# 詞嵌入層\n",
    "# 經過詞嵌入層的轉換，兩個新聞標題都變成\n",
    "# 一個詞向量的序列，而每個詞向量的維度\n",
    "# 為 256\n",
    "embedding_layer = Embedding(\n",
    "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "top_embedded = embedding_layer(\n",
    "    top_input)\n",
    "bm_embedded = embedding_layer(\n",
    "    bm_input)\n",
    "\n",
    "# LSTM 層\n",
    "# 兩個新聞標題經過此層後\n",
    "# 為一個 128 維度向量\n",
    "shared_lstm = LSTM(NUM_LSTM_UNITS)\n",
    "top_output = shared_lstm(top_embedded)\n",
    "bm_output = shared_lstm(bm_embedded)\n",
    "\n",
    "# 串接層將兩個新聞標題的結果串接單一向量\n",
    "# 方便跟全連結層相連\n",
    "merged = concatenate(\n",
    "    [top_output, bm_output], \n",
    "    axis=-1)\n",
    "\n",
    "# 全連接層搭配 Softmax Activation\n",
    "# 可以回傳 3 個成對標題\n",
    "# 屬於各類別的可能機率\n",
    "dense =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='softmax')\n",
    "predictions = dense(merged)\n",
    "\n",
    "# 我們的模型就是將數字序列的輸入，轉換\n",
    "# 成 3 個分類的機率的所有步驟 / 層的總和\n",
    "model = Model(\n",
    "    inputs=[top_input, bm_input], \n",
    "    outputs=predictions)\n",
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(\n",
    "    model, \n",
    "    to_file='model.png', \n",
    "    show_shapes=True, \n",
    "    show_layer_names=False, \n",
    "    rankdir='LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
